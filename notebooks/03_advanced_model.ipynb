{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef667b4",
   "metadata": {},
   "source": [
    "# Advanced Mental Health Text Classification\n",
    "\n",
    "This notebook explores advanced techniques to improve upon the baseline SVM model.\n",
    "\n",
    "## Techniques to Explore:\n",
    "1. **Ensemble Methods** - Combine multiple models\n",
    "2. **Feature Engineering** - Better text preprocessing\n",
    "3. **Hyperparameter Tuning** - Optimize model parameters\n",
    "4. **BERT/Transformers** - Use pre-trained language models\n",
    "5. **Cross-Validation** - Better model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80986f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for advanced modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the data (same as baseline)\n",
    "df = pd.read_csv(\"../data/cleaned_data.csv\")\n",
    "print(f\"Dataset loaded: {df.shape}\")\n",
    "print(f\"Class distribution:\\n{df['target'].value_counts().sort_index()}\")\n",
    "\n",
    "# Prepare data\n",
    "X = df['content']\n",
    "y = df['target']\n",
    "class_names = [\"Stress\", \"Depression\", \"Bipolar\", \"Personality\", \"Anxiety\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dda384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Feature Engineering\n",
    "print(\"=== ADVANCED FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Create multiple TF-IDF vectorizers with different configurations\n",
    "vectorizers = {\n",
    "    'basic': TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,1)),\n",
    "    'bigrams': TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2)),\n",
    "    'trigrams': TfidfVectorizer(max_features=7000, stop_words='english', ngram_range=(1,3)),\n",
    "    'advanced': TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        stop_words='english', \n",
    "        ngram_range=(1,2),\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Test each vectorizer with baseline SVM\n",
    "vectorizer_results = {}\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    print(f\"\\nTesting {name} vectorizer...\")\n",
    "    \n",
    "    # Transform data\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Train SVM\n",
    "    svm = LinearSVC(random_state=42, max_iter=2000)\n",
    "    svm.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    pred = svm.predict(X_test_vec)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    vectorizer_results[name] = accuracy\n",
    "    \n",
    "    print(f\"{name} accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Feature matrix shape: {X_train_vec.shape}\")\n",
    "\n",
    "# Select best vectorizer\n",
    "best_vectorizer_name = max(vectorizer_results, key=vectorizer_results.get)\n",
    "best_vectorizer = vectorizers[best_vectorizer_name]\n",
    "print(f\"\\nüèÜ BEST VECTORIZER: {best_vectorizer_name} (Accuracy: {vectorizer_results[best_vectorizer_name]:.4f})\")\n",
    "\n",
    "# Use best vectorizer for remaining experiments\n",
    "X_train_tfidf = best_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = best_vectorizer.transform(X_test)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6582fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "print(\"=== HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "# Define parameter grids for different models\n",
    "param_grids = {\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'max_iter': [1000, 2000, 3000],\n",
    "        'dual': [False]  # Better for n_samples > n_features\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'max_iter': [500, 1000, 2000],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Models to tune\n",
    "models_to_tune = {\n",
    "    'SVM': LinearSVC(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "tuned_models = {}\n",
    "best_scores = {}\n",
    "\n",
    "for model_name, model in models_to_tune.items():\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grids[model_name], \n",
    "        cv=3,  # 3-fold CV for speed\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Store results\n",
    "    tuned_models[model_name] = grid_search.best_estimator_\n",
    "    best_scores[model_name] = grid_search.best_score_\n",
    "    \n",
    "    print(f\"Best {model_name} parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best {model_name} CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ HYPERPARAMETER TUNING RESULTS:\")\n",
    "for model_name, score in best_scores.items():\n",
    "    print(f\"{model_name}: {score:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52047626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Methods\n",
    "print(\"=== ENSEMBLE METHODS ===\")\n",
    "\n",
    "# Test tuned models individually first\n",
    "individual_results = {}\n",
    "for model_name, model in tuned_models.items():\n",
    "    pred = model.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    individual_results[model_name] = accuracy\n",
    "    print(f\"Tuned {model_name} accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Create ensemble combinations\n",
    "ensemble_configs = {\n",
    "    'Voting_Hard': VotingClassifier([\n",
    "        ('svm', tuned_models['SVM']),\n",
    "        ('lr', tuned_models['LogisticRegression']),\n",
    "        ('rf', tuned_models['RandomForest'])\n",
    "    ], voting='hard'),\n",
    "    \n",
    "    'Voting_Soft': VotingClassifier([\n",
    "        ('lr', tuned_models['LogisticRegression']),\n",
    "        ('rf', tuned_models['RandomForest'])\n",
    "    ], voting='soft'),  # SVM doesn't support soft voting by default\n",
    "    \n",
    "    'Best_Two': VotingClassifier([\n",
    "        ('svm', tuned_models['SVM']),\n",
    "        ('lr', tuned_models['LogisticRegression'])\n",
    "    ], voting='hard')\n",
    "}\n",
    "\n",
    "# Train and evaluate ensemble models\n",
    "ensemble_results = {}\n",
    "print(f\"\\nEnsemble Results:\")\n",
    "for ensemble_name, ensemble in ensemble_configs.items():\n",
    "    print(f\"\\nTraining {ensemble_name}...\")\n",
    "    ensemble.fit(X_train_tfidf, y_train)\n",
    "    pred = ensemble.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    ensemble_results[ensemble_name] = accuracy\n",
    "    print(f\"{ensemble_name} accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Find overall best model\n",
    "all_results = {**individual_results, **ensemble_results}\n",
    "best_model_name = max(all_results, key=all_results.get)\n",
    "best_accuracy = all_results[best_model_name]\n",
    "\n",
    "print(f\"\\nüèÜ BEST OVERALL MODEL: {best_model_name}\")\n",
    "print(f\"üéØ BEST ACCURACY: {best_accuracy:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e19c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Interpretability Analysis\n",
    "print(\"=== MODEL INTERPRETABILITY ===\")\n",
    "\n",
    "# Use the best performing model for interpretability\n",
    "if best_model_name in tuned_models:\n",
    "    best_model = tuned_models[best_model_name]\n",
    "    model_type = \"individual\"\n",
    "else:\n",
    "    best_model = ensemble_configs[best_model_name]\n",
    "    model_type = \"ensemble\"\n",
    "\n",
    "print(f\"Analyzing {best_model_name} ({model_type} model)\")\n",
    "\n",
    "# Feature importance analysis (works for SVM and Logistic Regression)\n",
    "if hasattr(best_model, 'coef_') and model_type == \"individual\":\n",
    "    print(f\"\\nüìä FEATURE IMPORTANCE ANALYSIS\")\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = best_vectorizer.get_feature_names_out()\n",
    "    coefficients = best_model.coef_\n",
    "    \n",
    "    # Top features for each class\n",
    "    n_top_features = 10\n",
    "    print(f\"\\nTop {n_top_features} most important features for each class:\")\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"\\nüîç {class_name.upper()}:\")\n",
    "        \n",
    "        # Get top positive features (most predictive of this class)\n",
    "        top_positive_idx = np.argsort(coefficients[i])[-n_top_features:]\n",
    "        print(\"  Most predictive words:\")\n",
    "        for idx in reversed(top_positive_idx):\n",
    "            print(f\"    {feature_names[idx]}: {coefficients[i][idx]:.3f}\")\n",
    "        \n",
    "        # Get top negative features (most predictive of NOT this class)\n",
    "        top_negative_idx = np.argsort(coefficients[i])[:5]\n",
    "        print(\"  Least predictive words:\")\n",
    "        for idx in top_negative_idx:\n",
    "            print(f\"    {feature_names[idx]}: {coefficients[i][idx]:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Feature importance not available for ensemble models\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b35a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis\n",
    "print(\"=== ERROR ANALYSIS ===\")\n",
    "\n",
    "# Get predictions from best model\n",
    "if model_type == \"individual\":\n",
    "    best_predictions = best_model.predict(X_test_tfidf)\n",
    "else:\n",
    "    best_predictions = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "# Plot enhanced confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, \n",
    "            annot=True, \n",
    "            fmt=\"d\", \n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            square=True)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}\\nAccuracy: {best_accuracy:.4f}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed error analysis\n",
    "print(f\"\\nüîç DETAILED ERROR ANALYSIS:\")\n",
    "print(f\"Total test samples: {len(y_test)}\")\n",
    "print(f\"Correct predictions: {(best_predictions == y_test).sum()}\")\n",
    "print(f\"Incorrect predictions: {(best_predictions != y_test).sum()}\")\n",
    "\n",
    "# Per-class analysis\n",
    "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "print(f\"\\nPer-class accuracy:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"  {class_name}: {per_class_accuracy[i]:.4f}\")\n",
    "\n",
    "# Most common misclassifications\n",
    "print(f\"\\nMost common misclassifications:\")\n",
    "misclass_pairs = []\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if i != j and cm[i][j] > 0:\n",
    "            misclass_pairs.append((class_names[i], class_names[j], cm[i][j]))\n",
    "\n",
    "misclass_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "for true_label, pred_label, count in misclass_pairs[:10]:\n",
    "    print(f\"  {true_label} ‚Üí {pred_label}: {count} cases\")\n",
    "\n",
    "# Sample misclassified examples\n",
    "print(f\"\\nüìù SAMPLE MISCLASSIFIED EXAMPLES:\")\n",
    "misclassified_indices = np.where(best_predictions != y_test)[0]\n",
    "sample_size = min(5, len(misclassified_indices))\n",
    "\n",
    "for i in range(sample_size):\n",
    "    idx = misclassified_indices[i]\n",
    "    original_idx = X_test.index[idx]\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  True class: {class_names[y_test.iloc[idx]]}\")\n",
    "    print(f\"  Predicted: {class_names[best_predictions[idx]]}\")\n",
    "    print(f\"  Text: {X_test.iloc[idx][:200]}...\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Analysis\n",
    "print(\"=== CROSS-VALIDATION ANALYSIS ===\")\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "cv_folds = 5\n",
    "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Test top 3 models with CV\n",
    "top_models = dict(sorted(all_results.items(), key=lambda x: x[1], reverse=True)[:3])\n",
    "\n",
    "cv_results = {}\n",
    "for model_name, _ in top_models.items():\n",
    "    print(f\"\\nCross-validating {model_name}...\")\n",
    "    \n",
    "    if model_name in tuned_models:\n",
    "        model = tuned_models[model_name]\n",
    "    else:\n",
    "        model = ensemble_configs[model_name]\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=skf, scoring='accuracy')\n",
    "    \n",
    "    cv_results[model_name] = {\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std(),\n",
    "        'scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"  Individual folds: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "\n",
    "# Visualize CV results\n",
    "plt.figure(figsize=(12, 6))\n",
    "model_names = list(cv_results.keys())\n",
    "means = [cv_results[name]['mean'] for name in model_names]\n",
    "stds = [cv_results[name]['std'] for name in model_names]\n",
    "\n",
    "plt.bar(model_names, means, yerr=stds, capsize=5, alpha=0.7, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "plt.title('Cross-Validation Results (5-Fold)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xlabel('Models', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    plt.text(i, mean + std + 0.01, f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ MOST STABLE MODEL (lowest std): {min(cv_results.keys(), key=lambda x: cv_results[x]['std'])}\")\n",
    "print(f\"üèÜ HIGHEST CV ACCURACY: {max(cv_results.keys(), key=lambda x: cv_results[x]['mean'])}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e97b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Summary & Deployment\n",
    "print(\"=== FINAL MODEL COMPARISON & DEPLOYMENT ===\")\n",
    "\n",
    "# Create comprehensive results summary\n",
    "results_summary = pd.DataFrame([\n",
    "    {'Model': 'Baseline SVM (from previous notebook)', 'Test Accuracy': 0.8129, 'CV Mean': 'N/A', 'CV Std': 'N/A'},\n",
    "])\n",
    "\n",
    "# Add current results\n",
    "for model_name, test_acc in all_results.items():\n",
    "    cv_mean = cv_results.get(model_name, {}).get('mean', 'N/A')\n",
    "    cv_std = cv_results.get(model_name, {}).get('std', 'N/A')\n",
    "    \n",
    "    results_summary = pd.concat([results_summary, pd.DataFrame([{\n",
    "        'Model': model_name,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'CV Mean': cv_mean if cv_mean != 'N/A' else 'N/A',\n",
    "        'CV Std': cv_std if cv_std != 'N/A' else 'N/A'\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "# Sort by test accuracy\n",
    "results_summary = results_summary.sort_values('Test Accuracy', ascending=False)\n",
    "print(\"üìä COMPREHENSIVE MODEL COMPARISON:\")\n",
    "print(results_summary.to_string(index=False))\n",
    "\n",
    "# Calculate improvement\n",
    "baseline_accuracy = 0.8129\n",
    "best_improvement = best_accuracy - baseline_accuracy\n",
    "print(f\"\\nüöÄ IMPROVEMENT OVER BASELINE:\")\n",
    "print(f\"  Baseline SVM: {baseline_accuracy:.4f}\")\n",
    "print(f\"  Best Model ({best_model_name}): {best_accuracy:.4f}\")\n",
    "print(f\"  Improvement: +{best_improvement:.4f} ({best_improvement*100:.2f}%)\")\n",
    "\n",
    "# Save the best model\n",
    "print(f\"\\nüíæ SAVING BEST MODEL...\")\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Save best model and vectorizer\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = f\"../models/advanced_{best_model_name.lower().replace(' ', '_')}_model_{timestamp}.pkl\"\n",
    "vectorizer_filename = f\"../models/advanced_tfidf_vectorizer_{timestamp}.pkl\"\n",
    "\n",
    "# Get the actual model object\n",
    "if model_type == \"individual\":\n",
    "    final_model = best_model\n",
    "else:\n",
    "    final_model = best_model\n",
    "\n",
    "joblib.dump(final_model, model_filename)\n",
    "joblib.dump(best_vectorizer, vectorizer_filename)\n",
    "\n",
    "print(f\"‚úÖ Model saved: {model_filename}\")\n",
    "print(f\"‚úÖ Vectorizer saved: {vectorizer_filename}\")\n",
    "\n",
    "# Create prediction function for the advanced model\n",
    "def predict_mental_health_advanced(text):\n",
    "    \"\"\"Advanced prediction function with confidence scores\"\"\"\n",
    "    text_tfidf = best_vectorizer.transform([text])\n",
    "    prediction = final_model.predict(text_tfidf)[0]\n",
    "    \n",
    "    # Get confidence scores (different methods for different models)\n",
    "    if hasattr(final_model, 'predict_proba'):\n",
    "        probabilities = final_model.predict_proba(text_tfidf)[0]\n",
    "        confidence_scores = dict(zip(class_names, probabilities))\n",
    "    elif hasattr(final_model, 'decision_function'):\n",
    "        decision_scores = final_model.decision_function(text_tfidf)[0]\n",
    "        # Convert to probabilities using softmax\n",
    "        exp_scores = np.exp(decision_scores - np.max(decision_scores))\n",
    "        probabilities = exp_scores / np.sum(exp_scores)\n",
    "        confidence_scores = dict(zip(class_names, probabilities))\n",
    "    else:\n",
    "        # Fallback for ensemble methods\n",
    "        confidence_scores = {class_name: 0.2 for class_name in class_names}\n",
    "        confidence_scores[class_names[prediction]] = 0.8\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': class_names[prediction],\n",
    "        'class_number': prediction,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'model_used': best_model_name\n",
    "    }\n",
    "\n",
    "# Test the advanced model\n",
    "test_text = \"I've been feeling really anxious lately and having panic attacks\"\n",
    "result = predict_mental_health_advanced(test_text)\n",
    "\n",
    "print(f\"\\nüß™ TESTING ADVANCED MODEL:\")\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Predicted: {result['predicted_class']}\")\n",
    "print(f\"Model Used: {result['model_used']}\")\n",
    "print(f\"Confidence Scores:\")\n",
    "for class_name, score in result['confidence_scores'].items():\n",
    "    print(f\"  {class_name}: {score:.3f}\")\n",
    "\n",
    "print(f\"\\nüéâ ADVANCED MODEL READY!\")\n",
    "print(f\"Final Performance: {best_accuracy:.4f} accuracy\")\n",
    "print(f\"Improvement: +{best_improvement*100:.2f}% over baseline\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3641df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 1: Ensemble Methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data\n",
    "X = df['content']\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Vectorize\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = VotingClassifier([\n",
    "    ('lr', LogisticRegression(max_iter=500, random_state=42)),\n",
    "    ('nb', MultinomialNB()),\n",
    "    ('svm', LinearSVC(random_state=42)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "], voting='hard')\n",
    "\n",
    "ensemble.fit(X_train_tfidf, y_train)\n",
    "ensemble_pred = ensemble.predict(X_test_tfidf)\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_pred)\n",
    "\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"Improvement over baseline: {ensemble_accuracy - 0.8129:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 2: Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Tune SVM parameters\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'max_iter': [1000, 2000, 3000]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(LinearSVC(random_state=42), svm_params, cv=5, scoring='accuracy')\n",
    "svm_grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f\"Best SVM parameters: {svm_grid.best_params_}\")\n",
    "print(f\"Best SVM CV score: {svm_grid.best_score_:.4f}\")\n",
    "\n",
    "# Test best model\n",
    "best_svm = svm_grid.best_estimator_\n",
    "best_svm_pred = best_svm.predict(X_test_tfidf)\n",
    "best_svm_accuracy = accuracy_score(y_test, best_svm_pred)\n",
    "print(f\"Tuned SVM Test Accuracy: {best_svm_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f727297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 3: Better Feature Engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "def advanced_text_preprocessing(text):\n",
    "    \"\"\"Advanced text preprocessing\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, mentions, hashtags\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Keep only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_clean = X_train.apply(advanced_text_preprocessing)\n",
    "X_test_clean = X_test.apply(advanced_text_preprocessing)\n",
    "\n",
    "# Advanced TF-IDF\n",
    "tfidf_advanced = TfidfVectorizer(\n",
    "    max_features=10000,    # More features\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 3),    # Include trigrams\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True     # Use log normalization\n",
    ")\n",
    "\n",
    "X_train_advanced = tfidf_advanced.fit_transform(X_train_clean)\n",
    "X_test_advanced = tfidf_advanced.transform(X_test_clean)\n",
    "\n",
    "# Test with advanced features\n",
    "svm_advanced = LinearSVC(random_state=42, max_iter=2000)\n",
    "svm_advanced.fit(X_train_advanced, y_train)\n",
    "advanced_pred = svm_advanced.predict(X_test_advanced)\n",
    "advanced_accuracy = accuracy_score(y_test, advanced_pred)\n",
    "\n",
    "print(f\"Advanced Features Accuracy: {advanced_accuracy:.4f}\")\n",
    "print(f\"Feature matrix shape: {X_train_advanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84345730",
   "metadata": {},
   "source": [
    "## Next: BERT/Transformer Models\n",
    "\n",
    "For even better performance, consider:\n",
    "1. **Hugging Face Transformers**\n",
    "2. **Fine-tuned BERT models**\n",
    "3. **Mental health specific pre-trained models**\n",
    "\n",
    "This would require additional libraries:\n",
    "```bash\n",
    "pip install transformers torch\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental_env (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
